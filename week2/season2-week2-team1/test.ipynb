{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_search import get_google_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_api import OpenAI_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI_API()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-attention is a mechanism used in deep learning models, particularly in natural language processing tasks like machine translation or sentiment analysis. It allows the model to focus or attend to specific parts of the input sequence when making predictions.\\n\\nIn a self-attention mechanism, each word in the input sequence interacts with every other word to capture the importance or relevance of each word with respect to other words. This is different from traditional attention mechanisms, which typically only attend to the context or relevant words in the input sequence.\\n\\nSelf-attention is achieved through a series of mathematical operations. First, the input sequence is transformed into three different vectors called key, query, and value vectors. These vectors capture different aspects of the input, with the key vector representing the identifying information, the query vector representing the information being searched for, and the value vector representing the relevant information.\\n\\nNext, the similarity or relevance between the query and key vectors is computed using a dot product or cosine similarity. This results in attention weights, which determine the importance of each word in the input sequence with respect to the query word.\\n\\nFinally, the attention weights are used to compute a weighted sum of the value vectors, where the weights indicate how much attention to assign to each value. This aggregated representation is then used in subsequent layers of the model for prediction.\\n\\nBy utilizing self-attention, deep learning models are able to capture relationships and dependencies between words in the input sequence. It allows for more contextualized representations, leading to improved accuracy and performance in various natural language processing tasks.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.chatgpt(\"Explain self-attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OpenAIObject at 0x10692fd70> JSON: {\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": null,\n",
      "  \"function_call\": {\n",
      "    \"name\": \"Search\",\n",
      "    \"arguments\": \"{\\n  \\\"query\\\": \\\"self-attention\\\"\\n}\"\n",
      "  }\n",
      "}\n",
      "Search {'query': 'self-attention'}\n",
      "{'search_metadata': {'id': '64c113e89bddf72360924230', 'status': 'Success', 'json_endpoint': 'https://serpapi.com/searches/179bfc7da5a90319/64c113e89bddf72360924230.json', 'created_at': '2023-07-26 12:39:04 UTC', 'processed_at': '2023-07-26 12:39:04 UTC', 'google_url': 'https://www.google.com/search?q=self-attention&oq=self-attention&hl=ko&gl=kr&sourceid=chrome&ie=UTF-8', 'raw_html_file': 'https://serpapi.com/searches/179bfc7da5a90319/64c113e89bddf72360924230.html', 'total_time_taken': 5.83}, 'search_parameters': {'engine': 'google', 'q': 'self-attention', 'google_domain': 'google.com', 'hl': 'ko', 'gl': 'kr', 'device': 'desktop'}, 'search_information': {'organic_results_state': 'Results for exact spelling', 'query_displayed': 'self-attention', 'total_results': 1470000000, 'time_taken_displayed': 0.41, 'menu_items': [{'position': 1, 'title': '이미지', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention&tbm=isch&source=lnms&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ0pQJegQIDBAB', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_images&gl=kr&google_domain=google.com&hl=ko&q=self-attention'}, {'position': 2, 'title': '동영상', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention&tbm=vid&source=lnms&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ0pQJegQICBAB', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_videos&gl=kr&google_domain=google.com&hl=ko&q=self-attention'}, {'position': 3, 'title': '도서', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention&tbm=bks&source=lnms&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ0pQJegQICRAB'}, {'position': 4, 'title': 'Attention 차이', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+attention+%EC%B0%A8%EC%9D%B4&uds=H4sIAAAAAAAA_-Ny5RJwLClJzSvJzM9TeLNhxZu5W4T4ilNz0nQTYcIGTEXyqCIKiWhaAKq7AvdHAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQICxAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+attention+%EC%B0%A8%EC%9D%B4'}, {'position': 5, 'title': 'Transformer', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Transformer+Self-Attention&uds=H4sIAAAAAAAA_-Oy5uIOKUrMK07LL8pNLRLiK07NSdNNLClJzSvJzM8zYCqSQpJWCAbJOsJkAR9AxUA9AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVxAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Transformer+Self-Attention'}, {'position': 6, 'title': '장점', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+%EC%9E%A5%EC%A0%90&uds=H4sIAAAAAAAA_-My5GJ7M2_pmwUThPiKU3PSdBNLSlLzSjLz8wyYikRRRRQgCgGG2z1CMwAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVRAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+%EC%9E%A5%EC%A0%90'}, {'position': 7, 'title': 'Mechanism', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+mechanism&uds=H4sIAAAAAAAA_-My5-L0TU3OSMzLLM4V4itOzUnTTSwpSc0ryczPM2AqkghGEVHIhakFAEAiSW85AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVBAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+mechanism'}, {'position': 8, 'title': 'Paper', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+paper&uds=H4sIAAAAAAAA_-PS52INSCxILRLiK07NSdNNLClJzSvJzM8zYCoSCUYRUSgAqQMAB3AUIzEAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVhAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+paper'}, {'position': 9, 'title': 'CNN', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+CNN&uds=H4sIAAAAAAAA_-PS5mJ29vMT4itOzUnTTSwpSc0ryczPM2AqEgpGEVEAqgIAbpSlYi0AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIUxAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+CNN'}]}, 'knowledge_graph': {'title': '주의 집중 (Attention)', 'type': '기계 학습', 'kgmid': '/g/11qpclnpwr', 'knowledge_graph_search_link': 'https://www.google.com/search?kgmid=/g/11qpclnpwr&hl=ko-KR&q=%EC%A3%BC%EC%9D%98%EC%A7%91%EC%A4%91+(%EA%B8%B0%EA%B3%84+%ED%95%99%EC%8A%B5)&kgs=024830f48690923c&shndl=0&source=sh/x/kp/m1/1', 'serpapi_knowledge_graph_search_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko-KR&kgmid=%2Fg%2F11qpclnpwr&q=%EC%A3%BC%EC%9D%98%EC%A7%91%EC%A4%91+%28%EA%B8%B0%EA%B3%84+%ED%95%99%EC%8A%B5%29', 'tabs': [{'text': 'Attention 차이', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+attention+%EC%B0%A8%EC%9D%B4&uds=H4sIAAAAAAAA_-Ny5RJwLClJzSvJzM9TeLNhxZu5W4T4ilNz0nQTYcIGTEXyqCIKiWhaAKq7AvdHAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQICxAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+attention+%EC%B0%A8%EC%9D%B4'}, {'text': 'Transformer', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Transformer+Self-Attention&uds=H4sIAAAAAAAA_-Oy5uIOKUrMK07LL8pNLRLiK07NSdNNLClJzSvJzM8zYCqSQpJWCAbJOsJkAR9AxUA9AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVxAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Transformer+Self-Attention'}, {'text': '장점', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+%EC%9E%A5%EC%A0%90&uds=H4sIAAAAAAAA_-My5GJ7M2_pmwUThPiKU3PSdBNLSlLzSjLz8wyYikRRRRQgCgGG2z1CMwAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVRAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+%EC%9E%A5%EC%A0%90'}, {'text': 'Mechanism', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+mechanism&uds=H4sIAAAAAAAA_-My5-L0TU3OSMzLLM4V4itOzUnTTSwpSc0ryczPM2AqkghGEVHIhakFAEAiSW85AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVBAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+mechanism'}, {'text': 'Paper', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+paper&uds=H4sIAAAAAAAA_-PS52INSCxILRLiK07NSdNNLClJzSvJzM8zYCoSCUYRUSgAqQMAB3AUIzEAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVhAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+paper'}, {'text': 'CNN', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+CNN&uds=H4sIAAAAAAAA_-PS5mJ29vMT4itOzUnTTSwpSc0ryczPM2AqEgpGEVEAqgIAbpSlYi0AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIUxAB&ictx=0', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+CNN'}], 'header_images': [{'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67c91dc7b1dd9caa0df20f229d4f4087ae.png', 'source': 'https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EC%A7%91%EC%A4%91_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)'}, {'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67a0fbbdae91044c318f618d5275065146.png', 'source': 'https://medium.com/machine-intelligence-and-deep-learning-lab/transformer-the-self-attention-mechanism-d7d853c2c621'}, {'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67f5cb0cc52e0cf203211febd5bbdcd232.png', 'source': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html'}, {'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67b630406e2e5dfff2c14d2cce5337e81a.png', 'source': 'https://theaisummer.com/self-attention/'}, {'image': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRa-V7xYO6dqLGj0G9Bf5RgEQu25Af_eADLThACkT9cLQ&s', 'source': 'https://wdprogrammer.tistory.com/72'}, {'image': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRX5EYVNa_GyIcHbqXXQwnESB9J7T7ZGpcuuliG-cliOw&s', 'source': 'https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified'}], 'description': '신경망에서 주의 집중은 인지심리학에서의 주의를 모방하여 고안된 기술이다. 주의 집중은 입력 데이터 중 일부의 효과를 증강시키며, 다른 일부를 감소시킨다. 이는 네트워크가 데이터 중 비중이 적지만 중요한 데이터에 더 집중하게 하기 위해서이다.', 'source': {'name': '위키백과', 'link': 'https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EC%A7%91%EC%A4%91_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)'}}, 'inline_images': [{'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C%252Fg%252F11qpclnpwr%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kQMe1xRcIx2RfZk9Wj4baU2vTPVmA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_B16BAhOEAE#imgrc=TtJ1ivkUjyRMaM', 'source': 'https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EC%A7%91%EC%A4%91_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd32d0a3cea02adfafaeddee02a26503696e962a4f0e8a8a11.png', 'original': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Attention-animated.gif/700px-Attention-animated.gif', 'title': 'upload.wikimedia.org/wikipedia/commons/thumb/0/05/...', 'source_name': 'ko.wikipedia.org'}, {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhLEAE#imgrc=vojKeqq8-5ok1M', 'source': 'https://medium.com/machine-intelligence-and-deep-learning-lab/transformer-the-self-attention-mechanism-d7d853c2c621', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd4e3b82bf565ee4eec887b743ca050d51ccc57724cbcd66e3.png', 'original': 'https://miro.medium.com/v2/resize:fit:975/1*vrSX_Ku3EmGPyqF_E-2_Vg.png', 'title': 'Transformer: The Self-Attention Mechanism | by Sudipto Baul ...', 'source_name': 'Medium'}, {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhKEAE#imgrc=AgaounI5HMLFRM', 'source': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd94d15b4b0b6a795cd4a676f71a33628bfc1d54ccd1d971c6.png', 'original': 'https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/transformer.png', 'title': 'Understanding and Coding the Self-Attention Mechanism of ...', 'source_name': 'Sebastian Raschka'}, {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhIEAE#imgrc=RijDSx0tMlOrHM', 'source': 'https://theaisummer.com/self-attention/', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd3b99203efc2123d7d27e8ae55117802158044a0db983eb85.png', 'original': 'https://theaisummer.com/static/e497f0d469418119f9db9c53b9851e61/b9460/self-attention-explained.png', 'title': 'Why multi-head self attention works: math, intuitions and 10 ...', 'source_name': 'AI Summer'}, {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhJEAE#imgrc=je4-41GyGAj7tM', 'source': 'https://wdprogrammer.tistory.com/72', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRa-V7xYO6dqLGj0G9Bf5RgEQu25Af_eADLThACkT9cLQ&s', 'original': 'http://jalammar.github.io/images/t/self-attention-output.png', 'title': 'NLP] Transformer 모델 분석 (Self-Attention)', 'source_name': 'Wide and Deep Programming - 티스토리'}, {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhHEAE#imgrc=4l1h3Hf-lpJj7M', 'source': 'https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRX5EYVNa_GyIcHbqXXQwnESB9J7T7ZGpcuuliG-cliOw&s', 'original': 'https://vaclavkosar.com/images/transformer-full-model.png', 'title': \"Transformer's Self-Attention Mechanism Simplified\", 'source_name': \"Vaclav Kosar's Software & Machine Learning Blog\"}], 'related_questions': [{'question': 'What is self-attention?', 'snippet': 'Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', 'title': 'The Transformer Attention Mechanism - MachineLearningMastery.com', 'date': '2022. 9. 15.', 'link': 'https://machinelearningmastery.com/the-transformer-attention-mechanism/', 'displayed_link': 'https://machinelearningmastery.com › the-transformer-att...', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/282737a9f02dd8b7f68e4eb6e8a14388769a2a15f6d631be75380bfaca119668.png', 'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVPU0trN05TVk5JTENsSnpTdkp6TTlUU0swb3lFbk1Td1N4QVEiLCJicyI6ImMtT1M0UklQejBnc1VjZ3NWaWhPelVuVFRTd3BTYzByeWN6UHM1ZDRLY3RseTJVQmswM0pURXRMTFVyTlMwNVZTRW90S1U5TnpWT0FxMVZJekV2QjBQNkVqMHVCU3pvOG94TFRiSVhTNHRRVWU0a1pQRnlHWERwZ0MxTHlVekVVNWFZbTVpbWtwS1lXS09Ta0poYmxaZWFsMjB0Y0N4SmdCQUEiLCJpZCI6ImZjXzEifQ==', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVPU0trN05TVk5JTENsSnpTdkp6TTlUU0swb3lFbk1Td1N4QVEiLCJicyI6ImMtT1M0UklQejBnc1VjZ3NWaWhPelVuVFRTd3BTYzByeWN6UHM1ZDRLY3RseTJVQmswM0pURXRMTFVyTlMwNVZTRW90S1U5TnpWT0FxMVZJekV2QjBQNkVqMHVCU3pvOG94TFRiSVhTNHRRVWU0a1pQRnlHWERwZ0MxTHlVekVVNWFZbTVpbWtwS1lXS09Ta0poYmxaZWFsMjB0Y0N4SmdCQUEiLCJpZCI6ImZjXzEifQ%3D%3D'}, {'question': 'What is difference between attention and self-attention?', 'snippet': 'Self-attention is a specific type of attention. The difference between regular attention and self-attention is that instead of relating an input to an output sequence, self-attention focuses on a single sequence. It allows the model to let a sequence learn information about itself.', 'title': 'Demystifying efficient self-attention | by Thomas van Dongen', 'date': '2022. 11. 7.', 'link': 'https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb', 'displayed_link': 'https://towardsdatascience.com › demystifying-efficient-s...', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/282737a9f02dd8b7f68e4eb6e8a143888659104a522329fcaf59f59820d43d35.png', 'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIGRpZmZlcmVuY2UgYmV0d2VlbiBhdHRlbnRpb24gYW5kIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVNeUw4OUlMRkhJTEZaSXlVeExTeTFLelV0T1ZVaEtMU2xQVGMxVFNDd3BTYzByeWN3SHN2SlNGSXBUYzlJUVFnQSIsImJzIjoiYy1PUzRSSVB6MGdzVWNnc1ZpaE96VW5UVFN3cFNjMHJ5Y3pQczVkNEtjdGx5MlVCazAzSlRFdExMVXJOUzA1VlNFb3RLVTlOelZPQXExVkl6RXZCMFA2RWowdUJTem84b3hMVGJJWFM0dFFVZTRrWlBGeUdYRHBnQzFMeVV6RVU1YVltNWlta3BLWVdLT1NrSmhibFplYWwyMHRjQ3hKZ0JBQSIsImlkIjoiZmNfMSJ9', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIGRpZmZlcmVuY2UgYmV0d2VlbiBhdHRlbnRpb24gYW5kIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVNeUw4OUlMRkhJTEZaSXlVeExTeTFLelV0T1ZVaEtMU2xQVGMxVFNDd3BTYzByeWN3SHN2SlNGSXBUYzlJUVFnQSIsImJzIjoiYy1PUzRSSVB6MGdzVWNnc1ZpaE96VW5UVFN3cFNjMHJ5Y3pQczVkNEtjdGx5MlVCazAzSlRFdExMVXJOUzA1VlNFb3RLVTlOelZPQXExVkl6RXZCMFA2RWowdUJTem84b3hMVGJJWFM0dFFVZTRrWlBGeUdYRHBnQzFMeVV6RVU1YVltNWlta3BLWVdLT1NrSmhibFplYWwyMHRjQ3hKZ0JBQSIsImlkIjoiZmNfMSJ9'}, {'question': 'Why is self-attention used?', 'snippet': \"In layman's terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\", 'title': 'Illustrated: Self-Attention - Towards Data Science', 'link': 'https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a', 'displayed_link': 'https://towardsdatascience.com › illustrated-self-attenti...', 'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS1WBUhsqaANySoaNVt4HZ_advEZtAcdMJNHx5-9lQSFw&s', 'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaHkgaXMgc2VsZi1hdHRlbnRpb24gdXNlZD8iLCJsayI6IkdocDNhSGtnYVhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnZFhObFpBIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0=', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaHkgaXMgc2VsZi1hdHRlbnRpb24gdXNlZD8iLCJsayI6IkdocDNhSGtnYVhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnZFhObFpBIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0%3D'}, {'question': 'What does self-attention mean deep learning?', 'snippet': \"First, let's define what “self-Attention” is. Cheng et al, in their paper named “Long Short-Term Memory-Networks for Machine Reading”, defined self-Attention as the mechanism of relating different positions of a single sequence or sentence in order to gain a more vivid representation.\", 'title': 'Attention Mechanism In Deep Learning - Analytics Vidhya', 'date': '2019. 11. 20.', 'link': 'https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/', 'displayed_link': 'https://www.analyticsvidhya.com › blog › 2019/11 › co...', 'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/282737a9f02dd8b7f68e4eb6e8a143886458bc63dc7acbcecddf67faf1784e3b.png', 'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGRvZXMgc2VsZi1hdHRlbnRpb24gbWVhbiBkZWVwIGxlYXJuaW5nPyIsImxrIjoiR2l0M2FHRjBJR1J2WlhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnYldWaGJpQmtaV1Z3SUd4bFlYSnVhVzVuIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0=', 'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGRvZXMgc2VsZi1hdHRlbnRpb24gbWVhbiBkZWVwIGxlYXJuaW5nPyIsImxrIjoiR2l0M2FHRjBJR1J2WlhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnYldWaGJpQmtaV1Z3SUd4bFlYSnVhVzVuIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0%3D'}], 'organic_results': [{'position': 1, 'title': '셀프 어텐션 동작 원리', 'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/', 'displayed_link': 'https://ratsgo.github.io › docs › tr_self_attention', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee05672e104f392d5129ad1b22bc7f8c0f2.png', 'snippet': '트랜스포머(transformer)의 핵심 구성요소는 셀프 어텐션(self attention)입니다. 이 글에서는 셀프 어텐션의 내부 동작 원리에 대해 살펴보겠습니다.', 'snippet_highlighted_words': ['self attention'], 'sitelinks': {'inline': [{'title': '모델 입력과 출력', 'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/#%EB%AA%A8%EB%8D%B8-%EC%9E%85%EB%A0%A5%EA%B3%BC-%EC%B6%9C%EB%A0%A5'}, {'title': '셀프 어텐션 내부 동작', 'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/#%EC%85%80%ED%94%84-%EC%96%B4%ED%85%90%EC%85%98-%EB%82%B4%EB%B6%80-%EB%8F%99%EC%9E%91'}, {'title': '멀티 헤드 어텐션', 'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/#%EB%A9%80%ED%8B%B0-%ED%97%A4%EB%93%9C-%EC%96%B4%ED%85%90%EC%85%98'}]}, 'about_this_result': {'source': {'description': '깃허브는 루비 온 레일스로 작성된 분산 버전 관리 툴인 깃 저장소 호스팅을 지원하는 웹 서비스이다. 깃허브는 영리적인 서비스와 오픈소스를 위한 무상 서비스를 모두 제공한다. 2009년의 깃 사용자 조사에 따르면 깃허브는 가장 인기있는 깃 저장소 호스팅 서비스이다.', 'source_info_link': 'https://ko.wikipedia.org/wiki/%EA%B9%83%ED%97%88%EB%B8%8C', 'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://ratsgo.github.io&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'related_keywords': ['셀프'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:FWo9d7viZdIJ:https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/&cd=10&hl=ko&ct=clnk&gl=kr', 'source': 'github.io'}, {'position': 2, 'title': '16-01 트랜스포머(Transformer) - 딥 러닝을 이용한 자연어 처리 ...', 'link': 'https://wikidocs.net/31379', 'displayed_link': 'https://wikidocs.net › ...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0152f53e1f10136b7be86de346ad1fe9a.png', 'snippet': '트랜스포머(Transformer)는 2017년 구글이 발표한 논문인 \"Attention is all you ... 그런데 어텐션 중에서는 셀프 어텐션(self-attention)이라는 것이 있습니다.', 'snippet_highlighted_words': ['self', 'attention'], 'about_this_result': {'source': {'description': 'Google에서 wikidocs.net의 첫 색인을 생성한 지 10년이 넘었습니다.', 'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://wikidocs.net&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:aJGtlb-k488J:https://wikidocs.net/31379&cd=11&hl=ko&ct=clnk&gl=kr', 'source': 'wikidocs.net'}, {'position': 3, 'title': '4-1. Transformer(Self Attention) [초등학생도 이해하는 자연어 ...', 'link': 'https://codingopera.tistory.com/43', 'displayed_link': 'https://codingopera.tistory.com › ...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0e30de1b618662c8927a0790ba2995b5e.png', 'date': '2022. 12. 13.', 'snippet': '1. Self Attention. Self Attention. 첫 번째 메커니즘은 Self Attention입니다. \"Self Attention\"이란 말 ...', 'snippet_highlighted_words': ['Self Attention', 'Self Attention', 'Self Attention', 'Self Attention'], 'about_this_result': {'source': {'description': '티스토리는 대한민국의 가입형/설치형 블로그 서비스다. 2006년 5월 25일, 다음커뮤니케이션과 태터앤컴퍼니가 함께 공동운영을 시작했으나, 서비스 시작 1년 2개월 만인 2007년 7월 10일에 모든 서비스의 운영권이 다음커뮤니케이션으로 이관되었다.', 'source_info_link': 'https://ko.wikipedia.org/wiki/%ED%8B%B0%EC%8A%A4%ED%86%A0%EB%A6%AC', 'icon': 'https://encrypted-tbn0.gstatic.com/faviconV2?url=https://codingopera.tistory.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:NSn5nD0OWEkJ:https://codingopera.tistory.com/43&cd=12&hl=ko&ct=clnk&gl=kr', 'source': 'tistory.com'}, {'position': 4, 'title': 'Illustrated: Self-Attention - Towards Data Science', 'link': 'https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a', 'displayed_link': 'https://towardsdatascience.com › illustrated-self-attenti...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee013446cba497541d33c23b527afa59d5b.png', 'snippet': \"In layman's terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“ ...\", 'snippet_highlighted_words': ['self', 'attention'], 'about_this_result': {'source': {'description': 'Google에서 towardsdatascience.com의 첫 색인을 October 2016에 생성했습니다.', 'icon': 'https://encrypted-tbn1.gstatic.com/faviconV2?url=https://towardsdatascience.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:yQcQUFPxnN8J:https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a&cd=22&hl=ko&ct=clnk&gl=kr', 'source': 'towardsdatascience.com'}, {'position': 5, 'title': '[NLP] Transformer 모델 분석 (Self-Attention)', 'link': 'https://wdprogrammer.tistory.com/72', 'displayed_link': 'https://wdprogrammer.tistory.com › ...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee09b25ff4d8f931ba60182d80688070648.png', 'date': '2020. 10. 11.', 'snippet': '현재 Attention is All you Need는 NLP를 한다면 반드시 읽어야 될 논문일 뿐만 아니라 인공지능을 연구한다면 반드시 읽어봐야 할 논문이 되었다.', 'snippet_highlighted_words': ['Attention'], 'about_this_result': {'source': {'description': '티스토리는 대한민국의 가입형/설치형 블로그 서비스다. 2006년 5월 25일, 다음커뮤니케이션과 태터앤컴퍼니가 함께 공동운영을 시작했으나, 서비스 시작 1년 2개월 만인 2007년 7월 10일에 모든 서비스의 운영권이 다음커뮤니케이션으로 이관되었다.', 'source_info_link': 'https://ko.wikipedia.org/wiki/%ED%8B%B0%EC%8A%A4%ED%86%A0%EB%A6%AC', 'icon': 'https://encrypted-tbn0.gstatic.com/faviconV2?url=https://wdprogrammer.tistory.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:lGRCQAurV-YJ:https://wdprogrammer.tistory.com/72&cd=23&hl=ko&ct=clnk&gl=kr', 'source': 'tistory.com'}, {'position': 6, 'title': 'Attention, Transformer(Self-Attention) - velog', 'link': 'https://velog.io/@idj7183/Attention-TransformerSelf-Attention', 'displayed_link': 'https://velog.io › Attention-TransformerSelf-Attention', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0b51b6c53426a775a771786b67cd7fdbe.png', 'date': '2022. 3. 7.', 'snippet': 'Attention, Transformer(Self-Attention)', 'snippet_highlighted_words': ['Self', 'Attention'], 'about_this_result': {'source': {'description': 'Google에서 velog.io의 첫 색인을 March 2018에 생성했습니다.', 'icon': 'https://encrypted-tbn1.gstatic.com/faviconV2?url=https://velog.io&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:QVNzLAfmOq8J:https://velog.io/%40idj7183/Attention-TransformerSelf-Attention&cd=24&hl=ko&ct=clnk&gl=kr', 'source': 'velog.io'}, {'position': 7, 'title': 'Understanding and Coding the Self-Attention Mechanism of ...', 'link': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html', 'displayed_link': 'https://sebastianraschka.com › blog › self-attention-fro...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee05d6c64b7d5a20cc0afa567baa25db8f3.png', 'date': '2023. 2. 9.', 'snippet': 'We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the ...', 'snippet_highlighted_words': ['self', 'attention'], 'about_this_result': {'source': {'description': 'Google에서 sebastianraschka.com의 첫 색인을 생성한 지 10년이 넘었습니다.', 'icon': 'https://encrypted-tbn3.gstatic.com/faviconV2?url=https://sebastianraschka.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:ETHZx9LvnuUJ:https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html&cd=25&hl=ko&ct=clnk&gl=kr', 'source': 'sebastianraschka.com'}, {'position': 8, 'title': 'Self-attention - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Self-attention', 'displayed_link': 'https://en.wikipedia.org › wiki › Self-attention', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee09666d12028871b986419eecdb4b745f0.png', 'snippet': 'Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a ...', 'snippet_highlighted_words': ['Self Attention'], 'about_this_result': {'source': {'description': '위키백과 또는 위키피디아는 누구나 자유롭게 쓸 수 있는 다언어판 인터넷 백과사전이다. 개방된 협업을 통해 위키 기반 편집 시스템을 사용하여 자발적인 위키백과 사용자 공동체가 작성하고 관리하고 있다.', 'source_info_link': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC', 'icon': 'https://encrypted-tbn1.gstatic.com/faviconV2?url=https://en.wikipedia.org&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:sb7wZ-lxSq8J:https://en.wikipedia.org/wiki/Self-attention&cd=26&hl=ko&ct=clnk&gl=kr', 'source': 'wikipedia.org'}, {'position': 9, 'title': '어텐션 메커니즘과 transfomer(self-attention)', 'link': 'https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225', 'displayed_link': 'https://medium.com › platfarm › 어텐션-메커니즘과-t...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0aa47a4a2712990474853c4fd689137dc.png', 'date': '2019. 3. 10.', 'snippet': 'attn_output, attention = self.attention_net(output, final_hidden_state) 을 통해 지금까지의 LSTM output과 LSTM State의 마지막 ...', 'snippet_highlighted_words': ['attention', 'self'], 'about_this_result': {'source': {'description': '미디엄은 에번 윌리엄스가 개발한 온라인 출판 플랫폼의 하나로, 2012년 8월 시작되었다. A 미디엄 코퍼레이션의 소유이다.', 'source_info_link': 'https://ko.wikipedia.org/wiki/%EB%AF%B8%EB%94%94%EC%97%84_(%EC%9B%B9%EC%82%AC%EC%9D%B4%ED%8A%B8)', 'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://medium.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:FE9S0HsUjYgJ:https://medium.com/platfarm/%25EC%2596%25B4%25ED%2585%2590%25EC%2585%2598-%25EB%25A9%2594%25EC%25BB%25A4%25EB%258B%2588%25EC%25A6%2598%25EA%25B3%25BC-transfomer-self-attention-842498fd3225&cd=27&hl=ko&ct=clnk&gl=kr', 'source': 'medium.com'}, {'position': 10, 'title': 'Attention is All you Need - NIPS papers', 'link': 'https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf', 'displayed_link': 'https://papers.neurips.cc › paper › 7181-attentio...', 'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee00480cf54f14763c34a248187b7b54587.png', 'date': 'A Vaswani 저술 82841회 인용', 'snippet': 'Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task ...', 'snippet_highlighted_words': ['Self', 'attention'], 'about_this_result': {'source': {'description': 'Google에서 papers.neurips.cc의 첫 색인을 April 2019에 생성했습니다.', 'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://papers.neurips.cc&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'}, 'keywords': ['self', 'attention'], 'languages': ['한국어'], 'regions': ['대한민국']}, 'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:Wasbh3ymR2YJ:https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf&cd=28&hl=ko&ct=clnk&gl=kr', 'source': 'neurips.cc'}], 'related_searches': [{'query': 'multi-head self-attention 설명', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=multi-head+self-attention+%EC%84%A4%EB%AA%85&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ1QJ6BAgxEAE'}, {'query': 'multi-head attention 장점', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=multi-head+attention+%EC%9E%A5%EC%A0%90&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ1QJ6BAguEAE'}, {'query': 'Transformer attention', 'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Transformer+attention&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ1QJ6BAgtEAE'}], 'pagination': {'current': 1, 'next': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=10&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8NMDegQIBRAW', 'other_pages': {'2': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=10&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAE', '3': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=20&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAG', '4': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=30&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAI', '5': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=40&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAK', '6': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=50&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAM', '7': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=60&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAO', '8': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=70&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAQ', '9': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=80&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAS', '10': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=90&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAU'}}, 'serpapi_pagination': {'current': 1, 'next_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=10', 'next': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=10', 'other_pages': {'2': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=10', '3': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=20', '4': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=30', '5': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=40', '6': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=50', '7': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=60', '8': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=70', '9': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=80', '10': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=90'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'search_metadata': {'id': '64c113e89bddf72360924230',\n",
       "  'status': 'Success',\n",
       "  'json_endpoint': 'https://serpapi.com/searches/179bfc7da5a90319/64c113e89bddf72360924230.json',\n",
       "  'created_at': '2023-07-26 12:39:04 UTC',\n",
       "  'processed_at': '2023-07-26 12:39:04 UTC',\n",
       "  'google_url': 'https://www.google.com/search?q=self-attention&oq=self-attention&hl=ko&gl=kr&sourceid=chrome&ie=UTF-8',\n",
       "  'raw_html_file': 'https://serpapi.com/searches/179bfc7da5a90319/64c113e89bddf72360924230.html',\n",
       "  'total_time_taken': 5.83},\n",
       " 'search_parameters': {'engine': 'google',\n",
       "  'q': 'self-attention',\n",
       "  'google_domain': 'google.com',\n",
       "  'hl': 'ko',\n",
       "  'gl': 'kr',\n",
       "  'device': 'desktop'},\n",
       " 'search_information': {'organic_results_state': 'Results for exact spelling',\n",
       "  'query_displayed': 'self-attention',\n",
       "  'total_results': 1470000000,\n",
       "  'time_taken_displayed': 0.41,\n",
       "  'menu_items': [{'position': 1,\n",
       "    'title': '이미지',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention&tbm=isch&source=lnms&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ0pQJegQIDBAB',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_images&gl=kr&google_domain=google.com&hl=ko&q=self-attention'},\n",
       "   {'position': 2,\n",
       "    'title': '동영상',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention&tbm=vid&source=lnms&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ0pQJegQICBAB',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_videos&gl=kr&google_domain=google.com&hl=ko&q=self-attention'},\n",
       "   {'position': 3,\n",
       "    'title': '도서',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention&tbm=bks&source=lnms&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ0pQJegQICRAB'},\n",
       "   {'position': 4,\n",
       "    'title': 'Attention 차이',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+attention+%EC%B0%A8%EC%9D%B4&uds=H4sIAAAAAAAA_-Ny5RJwLClJzSvJzM9TeLNhxZu5W4T4ilNz0nQTYcIGTEXyqCIKiWhaAKq7AvdHAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQICxAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+attention+%EC%B0%A8%EC%9D%B4'},\n",
       "   {'position': 5,\n",
       "    'title': 'Transformer',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Transformer+Self-Attention&uds=H4sIAAAAAAAA_-Oy5uIOKUrMK07LL8pNLRLiK07NSdNNLClJzSvJzM8zYCqSQpJWCAbJOsJkAR9AxUA9AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVxAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Transformer+Self-Attention'},\n",
       "   {'position': 6,\n",
       "    'title': '장점',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+%EC%9E%A5%EC%A0%90&uds=H4sIAAAAAAAA_-My5GJ7M2_pmwUThPiKU3PSdBNLSlLzSjLz8wyYikRRRRQgCgGG2z1CMwAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVRAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+%EC%9E%A5%EC%A0%90'},\n",
       "   {'position': 7,\n",
       "    'title': 'Mechanism',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+mechanism&uds=H4sIAAAAAAAA_-My5-L0TU3OSMzLLM4V4itOzUnTTSwpSc0ryczPM2AqkghGEVHIhakFAEAiSW85AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVBAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+mechanism'},\n",
       "   {'position': 8,\n",
       "    'title': 'Paper',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+paper&uds=H4sIAAAAAAAA_-PS52INSCxILRLiK07NSdNNLClJzSvJzM8zYCoSCUYRUSgAqQMAB3AUIzEAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVhAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+paper'},\n",
       "   {'position': 9,\n",
       "    'title': 'CNN',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+CNN&uds=H4sIAAAAAAAA_-PS5mJ29vMT4itOzUnTTSwpSc0ryczPM2AqEgpGEVEAqgIAbpSlYi0AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIUxAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+CNN'}]},\n",
       " 'knowledge_graph': {'title': '주의 집중 (Attention)',\n",
       "  'type': '기계 학습',\n",
       "  'kgmid': '/g/11qpclnpwr',\n",
       "  'knowledge_graph_search_link': 'https://www.google.com/search?kgmid=/g/11qpclnpwr&hl=ko-KR&q=%EC%A3%BC%EC%9D%98%EC%A7%91%EC%A4%91+(%EA%B8%B0%EA%B3%84+%ED%95%99%EC%8A%B5)&kgs=024830f48690923c&shndl=0&source=sh/x/kp/m1/1',\n",
       "  'serpapi_knowledge_graph_search_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko-KR&kgmid=%2Fg%2F11qpclnpwr&q=%EC%A3%BC%EC%9D%98%EC%A7%91%EC%A4%91+%28%EA%B8%B0%EA%B3%84+%ED%95%99%EC%8A%B5%29',\n",
       "  'tabs': [{'text': 'Attention 차이',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+attention+%EC%B0%A8%EC%9D%B4&uds=H4sIAAAAAAAA_-Ny5RJwLClJzSvJzM9TeLNhxZu5W4T4ilNz0nQTYcIGTEXyqCIKiWhaAKq7AvdHAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQICxAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+attention+%EC%B0%A8%EC%9D%B4'},\n",
       "   {'text': 'Transformer',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Transformer+Self-Attention&uds=H4sIAAAAAAAA_-Oy5uIOKUrMK07LL8pNLRLiK07NSdNNLClJzSvJzM8zYCqSQpJWCAbJOsJkAR9AxUA9AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVxAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Transformer+Self-Attention'},\n",
       "   {'text': '장점',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=self-attention+%EC%9E%A5%EC%A0%90&uds=H4sIAAAAAAAA_-My5GJ7M2_pmwUThPiKU3PSdBNLSlLzSjLz8wyYikRRRRQgCgGG2z1CMwAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVRAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention+%EC%9E%A5%EC%A0%90'},\n",
       "   {'text': 'Mechanism',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+mechanism&uds=H4sIAAAAAAAA_-My5-L0TU3OSMzLLM4V4itOzUnTTSwpSc0ryczPM2AqkghGEVHIhakFAEAiSW85AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVBAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+mechanism'},\n",
       "   {'text': 'Paper',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+paper&uds=H4sIAAAAAAAA_-PS52INSCxILRLiK07NSdNNLClJzSvJzM8zYCoSCUYRUSgAqQMAB3AUIzEAAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIVhAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+paper'},\n",
       "   {'text': 'CNN',\n",
       "    'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Self-attention+CNN&uds=H4sIAAAAAAAA_-PS5mJ29vMT4itOzUnTTSwpSc0ryczPM2AqEgpGEVEAqgIAbpSlYi0AAAA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQxKsJegQIUxAB&ictx=0',\n",
       "    'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=Self-attention+CNN'}],\n",
       "  'header_images': [{'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67c91dc7b1dd9caa0df20f229d4f4087ae.png',\n",
       "    'source': 'https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EC%A7%91%EC%A4%91_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)'},\n",
       "   {'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67a0fbbdae91044c318f618d5275065146.png',\n",
       "    'source': 'https://medium.com/machine-intelligence-and-deep-learning-lab/transformer-the-self-attention-mechanism-d7d853c2c621'},\n",
       "   {'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67f5cb0cc52e0cf203211febd5bbdcd232.png',\n",
       "    'source': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html'},\n",
       "   {'image': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/a7ddef9ca10fefd6fc54858e7b102d82cc4130748aa71c67b630406e2e5dfff2c14d2cce5337e81a.png',\n",
       "    'source': 'https://theaisummer.com/self-attention/'},\n",
       "   {'image': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRa-V7xYO6dqLGj0G9Bf5RgEQu25Af_eADLThACkT9cLQ&s',\n",
       "    'source': 'https://wdprogrammer.tistory.com/72'},\n",
       "   {'image': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRX5EYVNa_GyIcHbqXXQwnESB9J7T7ZGpcuuliG-cliOw&s',\n",
       "    'source': 'https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified'}],\n",
       "  'description': '신경망에서 주의 집중은 인지심리학에서의 주의를 모방하여 고안된 기술이다. 주의 집중은 입력 데이터 중 일부의 효과를 증강시키며, 다른 일부를 감소시킨다. 이는 네트워크가 데이터 중 비중이 적지만 중요한 데이터에 더 집중하게 하기 위해서이다.',\n",
       "  'source': {'name': '위키백과',\n",
       "   'link': 'https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EC%A7%91%EC%A4%91_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)'}},\n",
       " 'inline_images': [{'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C%252Fg%252F11qpclnpwr%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kQMe1xRcIx2RfZk9Wj4baU2vTPVmA&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_B16BAhOEAE#imgrc=TtJ1ivkUjyRMaM',\n",
       "   'source': 'https://ko.wikipedia.org/wiki/%EC%A3%BC%EC%9D%98_%EC%A7%91%EC%A4%91_(%EA%B8%B0%EA%B3%84_%ED%95%99%EC%8A%B5)',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd32d0a3cea02adfafaeddee02a26503696e962a4f0e8a8a11.png',\n",
       "   'original': 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Attention-animated.gif/700px-Attention-animated.gif',\n",
       "   'title': 'upload.wikimedia.org/wikipedia/commons/thumb/0/05/...',\n",
       "   'source_name': 'ko.wikipedia.org'},\n",
       "  {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhLEAE#imgrc=vojKeqq8-5ok1M',\n",
       "   'source': 'https://medium.com/machine-intelligence-and-deep-learning-lab/transformer-the-self-attention-mechanism-d7d853c2c621',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd4e3b82bf565ee4eec887b743ca050d51ccc57724cbcd66e3.png',\n",
       "   'original': 'https://miro.medium.com/v2/resize:fit:975/1*vrSX_Ku3EmGPyqF_E-2_Vg.png',\n",
       "   'title': 'Transformer: The Self-Attention Mechanism | by Sudipto Baul ...',\n",
       "   'source_name': 'Medium'},\n",
       "  {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhKEAE#imgrc=AgaounI5HMLFRM',\n",
       "   'source': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd94d15b4b0b6a795cd4a676f71a33628bfc1d54ccd1d971c6.png',\n",
       "   'original': 'https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/transformer.png',\n",
       "   'title': 'Understanding and Coding the Self-Attention Mechanism of ...',\n",
       "   'source_name': 'Sebastian Raschka'},\n",
       "  {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhIEAE#imgrc=RijDSx0tMlOrHM',\n",
       "   'source': 'https://theaisummer.com/self-attention/',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/6cec5a66c6003acd3b99203efc2123d7d27e8ae55117802158044a0db983eb85.png',\n",
       "   'original': 'https://theaisummer.com/static/e497f0d469418119f9db9c53b9851e61/b9460/self-attention-explained.png',\n",
       "   'title': 'Why multi-head self attention works: math, intuitions and 10 ...',\n",
       "   'source_name': 'AI Summer'},\n",
       "  {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhJEAE#imgrc=je4-41GyGAj7tM',\n",
       "   'source': 'https://wdprogrammer.tistory.com/72',\n",
       "   'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRa-V7xYO6dqLGj0G9Bf5RgEQu25Af_eADLThACkT9cLQ&s',\n",
       "   'original': 'http://jalammar.github.io/images/t/self-attention-output.png',\n",
       "   'title': 'NLP] Transformer 모델 분석 (Self-Attention)',\n",
       "   'source_name': 'Wide and Deep Programming - 티스토리'},\n",
       "  {'link': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&tbm=isch&source=iu&ictx=1&vet=1&fir=TtJ1ivkUjyRMaM%252CcDGqusmXZjt0JM%252C_%253BvojKeqq8-5ok1M%252ChrJgCa7xpqrv4M%252C_%253BAgaounI5HMLFRM%252CETHZx9LvnuWF_M%252C_%253BRijDSx0tMlOrHM%252C_HW8aSk-TrwmnM%252C_%253Bje4-41GyGAj7tM%252ClGRCQAurV-aLaM%252C_%253B4l1h3Hf-lpJj7M%252CmaL2jgzNcnUQlM%252C_&usg=AI4_-kReaxwXokw5FzfFKKsBFW-ro2qk3g&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ_h16BAhHEAE#imgrc=4l1h3Hf-lpJj7M',\n",
       "   'source': 'https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified',\n",
       "   'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRX5EYVNa_GyIcHbqXXQwnESB9J7T7ZGpcuuliG-cliOw&s',\n",
       "   'original': 'https://vaclavkosar.com/images/transformer-full-model.png',\n",
       "   'title': \"Transformer's Self-Attention Mechanism Simplified\",\n",
       "   'source_name': \"Vaclav Kosar's Software & Machine Learning Blog\"}],\n",
       " 'related_questions': [{'question': 'What is self-attention?',\n",
       "   'snippet': 'Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.',\n",
       "   'title': 'The Transformer Attention Mechanism - MachineLearningMastery.com',\n",
       "   'date': '2022. 9. 15.',\n",
       "   'link': 'https://machinelearningmastery.com/the-transformer-attention-mechanism/',\n",
       "   'displayed_link': 'https://machinelearningmastery.com › the-transformer-att...',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/282737a9f02dd8b7f68e4eb6e8a14388769a2a15f6d631be75380bfaca119668.png',\n",
       "   'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVPU0trN05TVk5JTENsSnpTdkp6TTlUU0swb3lFbk1Td1N4QVEiLCJicyI6ImMtT1M0UklQejBnc1VjZ3NWaWhPelVuVFRTd3BTYzByeWN6UHM1ZDRLY3RseTJVQmswM0pURXRMTFVyTlMwNVZTRW90S1U5TnpWT0FxMVZJekV2QjBQNkVqMHVCU3pvOG94TFRiSVhTNHRRVWU0a1pQRnlHWERwZ0MxTHlVekVVNWFZbTVpbWtwS1lXS09Ta0poYmxaZWFsMjB0Y0N4SmdCQUEiLCJpZCI6ImZjXzEifQ==',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVPU0trN05TVk5JTENsSnpTdkp6TTlUU0swb3lFbk1Td1N4QVEiLCJicyI6ImMtT1M0UklQejBnc1VjZ3NWaWhPelVuVFRTd3BTYzByeWN6UHM1ZDRLY3RseTJVQmswM0pURXRMTFVyTlMwNVZTRW90S1U5TnpWT0FxMVZJekV2QjBQNkVqMHVCU3pvOG94TFRiSVhTNHRRVWU0a1pQRnlHWERwZ0MxTHlVekVVNWFZbTVpbWtwS1lXS09Ta0poYmxaZWFsMjB0Y0N4SmdCQUEiLCJpZCI6ImZjXzEifQ%3D%3D'},\n",
       "  {'question': 'What is difference between attention and self-attention?',\n",
       "   'snippet': 'Self-attention is a specific type of attention. The difference between regular attention and self-attention is that instead of relating an input to an output sequence, self-attention focuses on a single sequence. It allows the model to let a sequence learn information about itself.',\n",
       "   'title': 'Demystifying efficient self-attention | by Thomas van Dongen',\n",
       "   'date': '2022. 11. 7.',\n",
       "   'link': 'https://towardsdatascience.com/demystifying-efficient-self-attention-b3de61b9b0fb',\n",
       "   'displayed_link': 'https://towardsdatascience.com › demystifying-efficient-s...',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/282737a9f02dd8b7f68e4eb6e8a143888659104a522329fcaf59f59820d43d35.png',\n",
       "   'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIGRpZmZlcmVuY2UgYmV0d2VlbiBhdHRlbnRpb24gYW5kIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVNeUw4OUlMRkhJTEZaSXlVeExTeTFLelV0T1ZVaEtMU2xQVGMxVFNDd3BTYzByeWN3SHN2SlNGSXBUYzlJUVFnQSIsImJzIjoiYy1PUzRSSVB6MGdzVWNnc1ZpaE96VW5UVFN3cFNjMHJ5Y3pQczVkNEtjdGx5MlVCazAzSlRFdExMVXJOUzA1VlNFb3RLVTlOelZPQXExVkl6RXZCMFA2RWowdUJTem84b3hMVGJJWFM0dFFVZTRrWlBGeUdYRHBnQzFMeVV6RVU1YVltNWlta3BLWVdLT1NrSmhibFplYWwyMHRjQ3hKZ0JBQSIsImlkIjoiZmNfMSJ9',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGlzIGRpZmZlcmVuY2UgYmV0d2VlbiBhdHRlbnRpb24gYW5kIHNlbGYtYXR0ZW50aW9uPyIsImxrIjoiYzVNeUw4OUlMRkhJTEZaSXlVeExTeTFLelV0T1ZVaEtMU2xQVGMxVFNDd3BTYzByeWN3SHN2SlNGSXBUYzlJUVFnQSIsImJzIjoiYy1PUzRSSVB6MGdzVWNnc1ZpaE96VW5UVFN3cFNjMHJ5Y3pQczVkNEtjdGx5MlVCazAzSlRFdExMVXJOUzA1VlNFb3RLVTlOelZPQXExVkl6RXZCMFA2RWowdUJTem84b3hMVGJJWFM0dFFVZTRrWlBGeUdYRHBnQzFMeVV6RVU1YVltNWlta3BLWVdLT1NrSmhibFplYWwyMHRjQ3hKZ0JBQSIsImlkIjoiZmNfMSJ9'},\n",
       "  {'question': 'Why is self-attention used?',\n",
       "   'snippet': \"In layman's terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\",\n",
       "   'title': 'Illustrated: Self-Attention - Towards Data Science',\n",
       "   'link': 'https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a',\n",
       "   'displayed_link': 'https://towardsdatascience.com › illustrated-self-attenti...',\n",
       "   'thumbnail': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS1WBUhsqaANySoaNVt4HZ_advEZtAcdMJNHx5-9lQSFw&s',\n",
       "   'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaHkgaXMgc2VsZi1hdHRlbnRpb24gdXNlZD8iLCJsayI6IkdocDNhSGtnYVhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnZFhObFpBIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0=',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaHkgaXMgc2VsZi1hdHRlbnRpb24gdXNlZD8iLCJsayI6IkdocDNhSGtnYVhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnZFhObFpBIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0%3D'},\n",
       "  {'question': 'What does self-attention mean deep learning?',\n",
       "   'snippet': \"First, let's define what “self-Attention” is. Cheng et al, in their paper named “Long Short-Term Memory-Networks for Machine Reading”, defined self-Attention as the mechanism of relating different positions of a single sequence or sentence in order to gain a more vivid representation.\",\n",
       "   'title': 'Attention Mechanism In Deep Learning - Analytics Vidhya',\n",
       "   'date': '2019. 11. 20.',\n",
       "   'link': 'https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/',\n",
       "   'displayed_link': 'https://www.analyticsvidhya.com › blog › 2019/11 › co...',\n",
       "   'thumbnail': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/282737a9f02dd8b7f68e4eb6e8a143886458bc63dc7acbcecddf67faf1784e3b.png',\n",
       "   'next_page_token': 'eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGRvZXMgc2VsZi1hdHRlbnRpb24gbWVhbiBkZWVwIGxlYXJuaW5nPyIsImxrIjoiR2l0M2FHRjBJR1J2WlhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnYldWaGJpQmtaV1Z3SUd4bFlYSnVhVzVuIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0=',\n",
       "   'serpapi_link': 'https://serpapi.com/search.json?device=desktop&engine=google_related_questions&google_domain=google.com&next_page_token=eyJvbnMiOiIxMDA0MSIsImZjIjoiRW9zQkNreEJUV3N5Ykd4TFlXOWFOREpGY0dKSFMwUjJUa3BFTTJOelNIQTFOMEpvY0ZoUWQwUkRaVkZXYVU5SWNrSnRkVnBqWm0xc2JGRTBUbWgwWjI1Vk9USkxTbkprZVhnd1YwNHpPVGxwRWhjM1VsQkNXazk2ZFV4WlExVjNZbXRRY0dScGJ6SkJOQm9pUVV4RlV6bDFUV0l6VXpKV1VWZzNNVlpSWDA4MWRtaGtWRXgxYjA1bVRsbHNRUSIsImZjdiI6IjMiLCJlaSI6IjdSUEJaT3p1TFlDVXdia1BwZGlvMkE0IiwicWMiOiJDZzV6Wld4bUlHRjBkR1Z1ZEdsdmJoQUFmWmkxNWo0IiwicXVlc3Rpb24iOiJXaGF0IGRvZXMgc2VsZi1hdHRlbnRpb24gbWVhbiBkZWVwIGxlYXJuaW5nPyIsImxrIjoiR2l0M2FHRjBJR1J2WlhNZ2MyVnNaaUJoZEhSbGJuUnBiMjRnYldWaGJpQmtaV1Z3SUd4bFlYSnVhVzVuIiwiYnMiOiJjLU9TNFJJUHowZ3NVY2dzVmloT3pVblRUU3dwU2MwcnljelBzNWQ0S2N0bHkyVUJrMDNKVEV0TExVck5TMDVWU0VvdEtVOU56Vk9BcTFWSXpFdkIwUDZFajB1QlN6bzhveExUYklYUzR0UVVlNGtaUEZ5R1hEcGdDMUx5VXpFVTVhWW01aW1rcEtZV0tPU2tKaGJsWmVhbDIwdGNDeEpnQkFBIiwiaWQiOiJmY18xIn0%3D'}],\n",
       " 'organic_results': [{'position': 1,\n",
       "   'title': '셀프 어텐션 동작 원리',\n",
       "   'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/',\n",
       "   'displayed_link': 'https://ratsgo.github.io › docs › tr_self_attention',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee05672e104f392d5129ad1b22bc7f8c0f2.png',\n",
       "   'snippet': '트랜스포머(transformer)의 핵심 구성요소는 셀프 어텐션(self attention)입니다. 이 글에서는 셀프 어텐션의 내부 동작 원리에 대해 살펴보겠습니다.',\n",
       "   'snippet_highlighted_words': ['self attention'],\n",
       "   'sitelinks': {'inline': [{'title': '모델 입력과 출력',\n",
       "      'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/#%EB%AA%A8%EB%8D%B8-%EC%9E%85%EB%A0%A5%EA%B3%BC-%EC%B6%9C%EB%A0%A5'},\n",
       "     {'title': '셀프 어텐션 내부 동작',\n",
       "      'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/#%EC%85%80%ED%94%84-%EC%96%B4%ED%85%90%EC%85%98-%EB%82%B4%EB%B6%80-%EB%8F%99%EC%9E%91'},\n",
       "     {'title': '멀티 헤드 어텐션',\n",
       "      'link': 'https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/#%EB%A9%80%ED%8B%B0-%ED%97%A4%EB%93%9C-%EC%96%B4%ED%85%90%EC%85%98'}]},\n",
       "   'about_this_result': {'source': {'description': '깃허브는 루비 온 레일스로 작성된 분산 버전 관리 툴인 깃 저장소 호스팅을 지원하는 웹 서비스이다. 깃허브는 영리적인 서비스와 오픈소스를 위한 무상 서비스를 모두 제공한다. 2009년의 깃 사용자 조사에 따르면 깃허브는 가장 인기있는 깃 저장소 호스팅 서비스이다.',\n",
       "     'source_info_link': 'https://ko.wikipedia.org/wiki/%EA%B9%83%ED%97%88%EB%B8%8C',\n",
       "     'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://ratsgo.github.io&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'related_keywords': ['셀프'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:FWo9d7viZdIJ:https://ratsgo.github.io/nlpbook/docs/language_model/tr_self_attention/&cd=10&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'github.io'},\n",
       "  {'position': 2,\n",
       "   'title': '16-01 트랜스포머(Transformer) - 딥 러닝을 이용한 자연어 처리 ...',\n",
       "   'link': 'https://wikidocs.net/31379',\n",
       "   'displayed_link': 'https://wikidocs.net › ...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0152f53e1f10136b7be86de346ad1fe9a.png',\n",
       "   'snippet': '트랜스포머(Transformer)는 2017년 구글이 발표한 논문인 \"Attention is all you ... 그런데 어텐션 중에서는 셀프 어텐션(self-attention)이라는 것이 있습니다.',\n",
       "   'snippet_highlighted_words': ['self', 'attention'],\n",
       "   'about_this_result': {'source': {'description': 'Google에서 wikidocs.net의 첫 색인을 생성한 지 10년이 넘었습니다.',\n",
       "     'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://wikidocs.net&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:aJGtlb-k488J:https://wikidocs.net/31379&cd=11&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'wikidocs.net'},\n",
       "  {'position': 3,\n",
       "   'title': '4-1. Transformer(Self Attention) [초등학생도 이해하는 자연어 ...',\n",
       "   'link': 'https://codingopera.tistory.com/43',\n",
       "   'displayed_link': 'https://codingopera.tistory.com › ...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0e30de1b618662c8927a0790ba2995b5e.png',\n",
       "   'date': '2022. 12. 13.',\n",
       "   'snippet': '1. Self Attention. Self Attention. 첫 번째 메커니즘은 Self Attention입니다. \"Self Attention\"이란 말 ...',\n",
       "   'snippet_highlighted_words': ['Self Attention',\n",
       "    'Self Attention',\n",
       "    'Self Attention',\n",
       "    'Self Attention'],\n",
       "   'about_this_result': {'source': {'description': '티스토리는 대한민국의 가입형/설치형 블로그 서비스다. 2006년 5월 25일, 다음커뮤니케이션과 태터앤컴퍼니가 함께 공동운영을 시작했으나, 서비스 시작 1년 2개월 만인 2007년 7월 10일에 모든 서비스의 운영권이 다음커뮤니케이션으로 이관되었다.',\n",
       "     'source_info_link': 'https://ko.wikipedia.org/wiki/%ED%8B%B0%EC%8A%A4%ED%86%A0%EB%A6%AC',\n",
       "     'icon': 'https://encrypted-tbn0.gstatic.com/faviconV2?url=https://codingopera.tistory.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:NSn5nD0OWEkJ:https://codingopera.tistory.com/43&cd=12&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'tistory.com'},\n",
       "  {'position': 4,\n",
       "   'title': 'Illustrated: Self-Attention - Towards Data Science',\n",
       "   'link': 'https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a',\n",
       "   'displayed_link': 'https://towardsdatascience.com › illustrated-self-attenti...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee013446cba497541d33c23b527afa59d5b.png',\n",
       "   'snippet': \"In layman's terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“ ...\",\n",
       "   'snippet_highlighted_words': ['self', 'attention'],\n",
       "   'about_this_result': {'source': {'description': 'Google에서 towardsdatascience.com의 첫 색인을 October 2016에 생성했습니다.',\n",
       "     'icon': 'https://encrypted-tbn1.gstatic.com/faviconV2?url=https://towardsdatascience.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:yQcQUFPxnN8J:https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a&cd=22&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'towardsdatascience.com'},\n",
       "  {'position': 5,\n",
       "   'title': '[NLP] Transformer 모델 분석 (Self-Attention)',\n",
       "   'link': 'https://wdprogrammer.tistory.com/72',\n",
       "   'displayed_link': 'https://wdprogrammer.tistory.com › ...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee09b25ff4d8f931ba60182d80688070648.png',\n",
       "   'date': '2020. 10. 11.',\n",
       "   'snippet': '현재 Attention is All you Need는 NLP를 한다면 반드시 읽어야 될 논문일 뿐만 아니라 인공지능을 연구한다면 반드시 읽어봐야 할 논문이 되었다.',\n",
       "   'snippet_highlighted_words': ['Attention'],\n",
       "   'about_this_result': {'source': {'description': '티스토리는 대한민국의 가입형/설치형 블로그 서비스다. 2006년 5월 25일, 다음커뮤니케이션과 태터앤컴퍼니가 함께 공동운영을 시작했으나, 서비스 시작 1년 2개월 만인 2007년 7월 10일에 모든 서비스의 운영권이 다음커뮤니케이션으로 이관되었다.',\n",
       "     'source_info_link': 'https://ko.wikipedia.org/wiki/%ED%8B%B0%EC%8A%A4%ED%86%A0%EB%A6%AC',\n",
       "     'icon': 'https://encrypted-tbn0.gstatic.com/faviconV2?url=https://wdprogrammer.tistory.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:lGRCQAurV-YJ:https://wdprogrammer.tistory.com/72&cd=23&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'tistory.com'},\n",
       "  {'position': 6,\n",
       "   'title': 'Attention, Transformer(Self-Attention) - velog',\n",
       "   'link': 'https://velog.io/@idj7183/Attention-TransformerSelf-Attention',\n",
       "   'displayed_link': 'https://velog.io › Attention-TransformerSelf-Attention',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0b51b6c53426a775a771786b67cd7fdbe.png',\n",
       "   'date': '2022. 3. 7.',\n",
       "   'snippet': 'Attention, Transformer(Self-Attention)',\n",
       "   'snippet_highlighted_words': ['Self', 'Attention'],\n",
       "   'about_this_result': {'source': {'description': 'Google에서 velog.io의 첫 색인을 March 2018에 생성했습니다.',\n",
       "     'icon': 'https://encrypted-tbn1.gstatic.com/faviconV2?url=https://velog.io&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:QVNzLAfmOq8J:https://velog.io/%40idj7183/Attention-TransformerSelf-Attention&cd=24&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'velog.io'},\n",
       "  {'position': 7,\n",
       "   'title': 'Understanding and Coding the Self-Attention Mechanism of ...',\n",
       "   'link': 'https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html',\n",
       "   'displayed_link': 'https://sebastianraschka.com › blog › self-attention-fro...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee05d6c64b7d5a20cc0afa567baa25db8f3.png',\n",
       "   'date': '2023. 2. 9.',\n",
       "   'snippet': 'We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the ...',\n",
       "   'snippet_highlighted_words': ['self', 'attention'],\n",
       "   'about_this_result': {'source': {'description': 'Google에서 sebastianraschka.com의 첫 색인을 생성한 지 10년이 넘었습니다.',\n",
       "     'icon': 'https://encrypted-tbn3.gstatic.com/faviconV2?url=https://sebastianraschka.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:ETHZx9LvnuUJ:https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html&cd=25&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'sebastianraschka.com'},\n",
       "  {'position': 8,\n",
       "   'title': 'Self-attention - Wikipedia',\n",
       "   'link': 'https://en.wikipedia.org/wiki/Self-attention',\n",
       "   'displayed_link': 'https://en.wikipedia.org › wiki › Self-attention',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee09666d12028871b986419eecdb4b745f0.png',\n",
       "   'snippet': 'Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a ...',\n",
       "   'snippet_highlighted_words': ['Self Attention'],\n",
       "   'about_this_result': {'source': {'description': '위키백과 또는 위키피디아는 누구나 자유롭게 쓸 수 있는 다언어판 인터넷 백과사전이다. 개방된 협업을 통해 위키 기반 편집 시스템을 사용하여 자발적인 위키백과 사용자 공동체가 작성하고 관리하고 있다.',\n",
       "     'source_info_link': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC',\n",
       "     'icon': 'https://encrypted-tbn1.gstatic.com/faviconV2?url=https://en.wikipedia.org&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:sb7wZ-lxSq8J:https://en.wikipedia.org/wiki/Self-attention&cd=26&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'wikipedia.org'},\n",
       "  {'position': 9,\n",
       "   'title': '어텐션 메커니즘과 transfomer(self-attention)',\n",
       "   'link': 'https://medium.com/platfarm/%EC%96%B4%ED%85%90%EC%85%98-%EB%A9%94%EC%BB%A4%EB%8B%88%EC%A6%98%EA%B3%BC-transfomer-self-attention-842498fd3225',\n",
       "   'displayed_link': 'https://medium.com › platfarm › 어텐션-메커니즘과-t...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee0aa47a4a2712990474853c4fd689137dc.png',\n",
       "   'date': '2019. 3. 10.',\n",
       "   'snippet': 'attn_output, attention = self.attention_net(output, final_hidden_state) 을 통해 지금까지의 LSTM output과 LSTM State의 마지막 ...',\n",
       "   'snippet_highlighted_words': ['attention', 'self'],\n",
       "   'about_this_result': {'source': {'description': '미디엄은 에번 윌리엄스가 개발한 온라인 출판 플랫폼의 하나로, 2012년 8월 시작되었다. A 미디엄 코퍼레이션의 소유이다.',\n",
       "     'source_info_link': 'https://ko.wikipedia.org/wiki/%EB%AF%B8%EB%94%94%EC%97%84_(%EC%9B%B9%EC%82%AC%EC%9D%B4%ED%8A%B8)',\n",
       "     'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://medium.com&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:FE9S0HsUjYgJ:https://medium.com/platfarm/%25EC%2596%25B4%25ED%2585%2590%25EC%2585%2598-%25EB%25A9%2594%25EC%25BB%25A4%25EB%258B%2588%25EC%25A6%2598%25EA%25B3%25BC-transfomer-self-attention-842498fd3225&cd=27&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'medium.com'},\n",
       "  {'position': 10,\n",
       "   'title': 'Attention is All you Need - NIPS papers',\n",
       "   'link': 'https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf',\n",
       "   'displayed_link': 'https://papers.neurips.cc › paper › 7181-attentio...',\n",
       "   'favicon': 'https://serpapi.com/searches/64c113e89bddf72360924230/images/c00a79d6edee4f16dd1e8fe24b868ee00480cf54f14763c34a248187b7b54587.png',\n",
       "   'date': 'A Vaswani 저술 82841회 인용',\n",
       "   'snippet': 'Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task ...',\n",
       "   'snippet_highlighted_words': ['Self', 'attention'],\n",
       "   'about_this_result': {'source': {'description': 'Google에서 papers.neurips.cc의 첫 색인을 April 2019에 생성했습니다.',\n",
       "     'icon': 'https://encrypted-tbn2.gstatic.com/faviconV2?url=https://papers.neurips.cc&client=ABOUT_THIS_RESULT&size=32&type=FAVICON&fallback_opts=TYPE,SIZE,URL'},\n",
       "    'keywords': ['self', 'attention'],\n",
       "    'languages': ['한국어'],\n",
       "    'regions': ['대한민국']},\n",
       "   'cached_page_link': 'https://webcache.googleusercontent.com/search?q=cache:Wasbh3ymR2YJ:https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf&cd=28&hl=ko&ct=clnk&gl=kr',\n",
       "   'source': 'neurips.cc'}],\n",
       " 'related_searches': [{'query': 'multi-head self-attention 설명',\n",
       "   'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=multi-head+self-attention+%EC%84%A4%EB%AA%85&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ1QJ6BAgxEAE'},\n",
       "  {'query': 'multi-head attention 장점',\n",
       "   'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=multi-head+attention+%EC%9E%A5%EC%A0%90&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ1QJ6BAguEAE'},\n",
       "  {'query': 'Transformer attention',\n",
       "   'link': 'https://www.google.com/search?sca_esv=551161701&hl=ko&gl=kr&q=Transformer+attention&sa=X&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ1QJ6BAgtEAE'}],\n",
       " 'pagination': {'current': 1,\n",
       "  'next': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=10&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8NMDegQIBRAW',\n",
       "  'other_pages': {'2': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=10&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAE',\n",
       "   '3': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=20&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAG',\n",
       "   '4': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=30&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAI',\n",
       "   '5': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=40&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAK',\n",
       "   '6': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=50&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAM',\n",
       "   '7': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=60&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAO',\n",
       "   '8': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=70&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAQ',\n",
       "   '9': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=80&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAS',\n",
       "   '10': 'https://www.google.com/search?q=self-attention&sca_esv=551161701&hl=ko&gl=kr&ei=7RPBZOzuLYCUwbkPpdio2A4&start=90&sa=N&ved=2ahUKEwismdqSsqyAAxUASjABHSUsCusQ8tMDegQIBRAU'}},\n",
       " 'serpapi_pagination': {'current': 1,\n",
       "  'next_link': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=10',\n",
       "  'next': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=10',\n",
       "  'other_pages': {'2': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=10',\n",
       "   '3': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=20',\n",
       "   '4': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=30',\n",
       "   '5': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=40',\n",
       "   '6': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=50',\n",
       "   '7': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=60',\n",
       "   '8': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=70',\n",
       "   '9': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=80',\n",
       "   '10': 'https://serpapi.com/search.json?device=desktop&engine=google&gl=kr&google_domain=google.com&hl=ko&q=self-attention&start=90'}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.use_tools(\"What is self-attention? Search google\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
